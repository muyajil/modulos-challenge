{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"modulos_challenge.ipynb","version":"0.3.2","provenance":[{"file_id":"1PPwYmfZJXUE-uVG4GWjsmJgAdMJ3_hy9","timestamp":1540540612295}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"STSukkjrXzOm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":120},"outputId":"3c7e7e63-8562-47eb-95d1-5efb38ba1ec0","executionInfo":{"status":"ok","timestamp":1540541309878,"user_tz":-120,"elapsed":31508,"user":{"displayName":"Mohammed Ajil","photoUrl":"https://lh4.googleusercontent.com/-uYzW-_MAh6Y/AAAAAAAAAAI/AAAAAAAAT6o/SYdHsLm0MeI/s64/photo.jpg","userId":"05321956665336598504"}}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","plt.rcParams['figure.figsize'] = [14, 5]\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from google.colab import drive\n","drive.mount('/drive')\n","BASE_PATH = '/drive/My Drive/02_coding/04_notebooks/01_modulos_challenge'"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /drive\n"],"name":"stdout"}]},{"metadata":{"id":"K2YlfaMKXzOq","colab_type":"text"},"cell_type":"markdown","source":["# Start\n","\n","- The first step in analyzing any dataset is to visualize and check out the behaviour of the variables first.\n","- As I understand the task in the 3 datasets we are looking at different kind of anomalies in the data. I guess the goal would be to have a model for each dataset predicting if there was an anomaly or not.\n","- A problem is the metric to decide if we have a good performance. Since we are in a unsupervised setting and we do not actually know what an anomaly is, we must resort to a lot of qualitative analysis and look at samples and the result. \n","- Therefore it is a good start to take out a few of the samples of each dataset and look at them."]},{"metadata":{"id":"1nPruaTRZ867","colab_type":"text"},"cell_type":"markdown","source":["## Train_Dataset_1.npy"]},{"metadata":{"scrolled":false,"id":"NCp_G3isXzOr","colab_type":"code","outputId":"e23a7e35-d319-42f5-89d8-a8449082e3c4","colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"status":"error","timestamp":1540541339626,"user_tz":-120,"elapsed":1803,"user":{"displayName":"Mohammed Ajil","photoUrl":"https://lh4.googleusercontent.com/-uYzW-_MAh6Y/AAAAAAAAAAI/AAAAAAAAT6o/SYdHsLm0MeI/s64/photo.jpg","userId":"05321956665336598504"}}},"cell_type":"code","source":["df = pd.DataFrame(np.load(BASE_PATH + 'data/Train_Dataset_1.npy'))\n","random_indices = np.random.randint(0, 1999, (10,))\n","\n","def plot_simple(data):\n","    sns.lineplot(x=range(1200), y=data)\n","    plt.ylim(0,1.3)\n","    plt.show()\n","\n","for x in random_indices:\n","    print(\"Checking out row {}\".format(x))\n","    plot_simple(np.array(df.loc[[x]]).squeeze())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Checking out row 281\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c3fa2c8b8fa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checking out row {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mplot_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-c3fa2c8b8fa0>\u001b[0m in \u001b[0;36mplot_simple\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'lineplot'"]}]},{"metadata":{"id":"gs0QPaodXzOw","colab_type":"text"},"cell_type":"markdown","source":["- We can instantly see that there are two different kind of modes:\n","    - In the first mode the variable we observe oscillates around 1 +/- $\\epsilon$, with different errors but the mean is definitely around 1.\n","    - In the second mode we can see clear outbreaks, the mean and error of the oscillation change drastically. \n","- Without any input from the client and without knowing what variables we are looking at it is difficult to state with certainty what is an anomaly and what not.\n","- So we need to make an assumption here, to know what we are looking for. However the results have to be understood as a result of this assumption, if we find out from the client or another way that this assumption is incorrect we need start back here with the new assumption.\n","- We will assume for the following the the outbreaks we observe in the second modes are the anomalies we are looking for.\n","- In cases like these where we can see that we have two different means and errors the next step would be to plot histograms together with the values of the values and see if we maybe can see the two different distributions."]},{"metadata":{"scrolled":false,"id":"JgzNrr7KXzOx","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_with_histo(data):\n","    plt.figure()\n","    plt.subplot(1,2,1)\n","    sns.lineplot(x=range(1200), y=data)\n","    plt.ylim(0,1.3)\n","    plt.subplot(1,2,2)\n","    sns.distplot(data)\n","    plt.xlim(0, 1.3)\n","    plt.ylim(0, 25)\n","    plt.show()\n","\n","random_indices = np.random.randint(0, 1999, (10,))\n","for x in random_indices:\n","    print(\"Checking out row {}\".format(x))\n","    plot_with_histo(np.array(df.loc[[x]]).squeeze())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k-WX-Sr-XzO1","colab_type":"text"},"cell_type":"markdown","source":["- In these plots we can quite clearly see that when we have an anomaly the distribution of the datapoints shows two modes.\n","- From the visualizations we can conclude that we can approximate these modes using gaussians.\n","- So now we have a strategy for a baseline predictor:\n","    - As a first step we can cluster the datapoints of each sample. Since we use gaussians to approximate the datapoints we can use gaussian mixture models to fit the datapoints.\n","    - As a second step we need to find out which of the modes is the anomaly. Any datapoint falling into this mode can be classified as an anomaly.\n","- This probabilistic approach is also good to combat false positives and negatives. The result of the clustering will be two probability distributions. For each datapoint we can then compute the probability of lying in a particular cluster. This allows us to use different strategies in classifying the datapoints, this is something that can be optimized also during operation.\n","- We are only looking to fit 1 or 2 Gaussian Modes per sample, therefore we can make an exhaustive search and visualize the results.\n","- We can choose the better fit using the BIC criterion.\n","- Now lets do that and look at the results we get."]},{"metadata":{"scrolled":false,"id":"M24l6mIYXzO2","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.mixture import GaussianMixture\n","\n","def plot_classified(sample, labels):\n","    plt.figure()\n","    \n","    data_0 = []\n","    data_1 = []\n","    for x in list(range(len(sample))):\n","        value = sample[x]\n","        if labels[x] == 0:\n","            data_0.append((x, value))\n","        else:\n","            data_1.append((x, value))\n","    \n","    if len(data_0) > 1:\n","        x_0, y_0 = zip(*data_0)\n","        print('Num datapoints in class 0: {}'.format(len(x_0)))\n","    if len(data_1) > 1:\n","        x_1, y_1 = zip(*data_1)\n","        print('Num datapoints in class 1: {}'.format(len(x_1)))\n","    \n","    # Simple plot of series with predicted labels\n","    plt.subplot(1,2,1)\n","    if len(data_0) > 1:\n","        sns.lineplot(x=x_0, y=y_0)\n","    if len(data_1) > 1:\n","        sns.lineplot(x=x_1, y=y_1)\n","    plt.ylim(-2,2.3)\n","    \n","    # Histogram of series with predicted labels\n","    plt.subplot(1,2,2)\n","    if len(data_0) > 1:\n","        sns.distplot(y_0)\n","    if len(data_1) > 1:\n","        sns.distplot(y_1)\n","    plt.xlim(-2,2.3)\n","    plt.ylim(0, 25)\n","    plt.show()\n","\n","def fit(sample, n_components):\n","    clf = GaussianMixture(n_components=n_components, covariance_type='full')\n","    labels = clf.fit_predict(sample.reshape(-1, 1))\n","    bic = clf.bic(sample.reshape(-1, 1))\n","    return bic, labels\n","\n","random_indices = np.random.randint(0, 1999, (10,))\n","for x in random_indices:\n","    print('================================================================================')\n","    print(\"Processing row {}\".format(x))\n","    sample = np.asarray(df.loc[[x]].transpose()).squeeze()\n","    bic_1, labels_1 = fit(sample, n_components=1)\n","    bic_2, labels_2 = fit(sample, n_components=2)\n","    \n","    if bic_1 < bic_2:\n","        plot_classified(sample, labels_1)\n","    else:\n","        plot_classified(sample, labels_2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nVJKLzvLXzO6","colab_type":"text"},"cell_type":"markdown","source":["- We can quickly see that we are not too bad in classifying the data points.\n","- Problems arise when the two distributions overlap. Then the model can not reliably detect the cluster.\n","- Since we are dealing with sequential data we can use this information. We need to do some feature engineering. For example we could try to predict the label of a datapoint from the past few datapoints. Like this we would kind of regularize the noisy results. We can extract these features and apply a simple SVM approach and look at what we get. We have to try out different window sizes.\n","- In the following we will only display cases with two modes, since these are the interesting ones."]},{"metadata":{"scrolled":false,"id":"apYktvIQXzO6","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","def create_features(series, window_size):\n","    x = []\n","    y = []\n","    for idx, label in enumerate(series[window_size:]):\n","        x.append(series[idx:idx+window_size-1])\n","        y.append(label)\n","    return np.asarray(x), np.asarray(y)\n","\n","def optimize_labels(labels):\n","    window_size = 10\n","    X, y = create_features(labels, window_size)\n","    if len(set(y)) == 1:\n","        return labels\n","    clf = SVC()\n","    clf.fit(X, y)\n","    labels_new = clf.predict(X)\n","    return np.append(labels[:window_size], labels_new)\n","        \n","random_indices = np.random.randint(0, 1999, (30,))\n","for x in random_indices:\n","    sample = np.asarray(df.loc[[x]].transpose()).squeeze()\n","    bic_1, labels_1 = fit(sample, n_components=1)\n","    bic_2, labels_2 = fit(sample, n_components=2)\n","    if bic_1 < bic_2:\n","        pass\n","    else:\n","        print('================================================================================')\n","        print(\"Processing row {}\".format(x))\n","        print(\"Optimized Labels\")\n","        plot_classified(sample, optimize_labels(labels_2))\n","        print(\"Not optimized labels\")\n","        plot_classified(sample, labels_2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3jlpg0ZRXzO_","colab_type":"text"},"cell_type":"markdown","source":["- When we compare the results of the optimized labels with the not optimized labels we can see that we can really improve the results by adding a time series classifier on top of the original model\n","\n","- However there are also problems. Sometimes the label optimizer maps all labels to 1 class. This happens mostly when we have two distributions with the same mean, but different variance.\n","- When optimizing the labels we already know that we are dealing with 2 classes. Therefore we do not want to map everything to one class. \n","\n","- Until now we did not apply any preprocessing to the data. Essentially we are not interested in what the exact distribution of the anomaly is, we just want to know when it happens. Therefore we can use transformations to try amplify the difference between the two distributions.\n","\n","- First we try a simple method. We keep a running window of means and then we amplify the difference between the mean and the value. What whe do in essence is we stretch the variance of the gaussians. The bigger the variance the bigger the increase, therefore the differences become more apparent."]},{"metadata":{"scrolled":false,"id":"5oOfI43wXzO_","colab_type":"code","colab":{}},"cell_type":"code","source":["def preprocess(series):\n","    window_size = 10\n","    preprocessed_series = []\n","    for idx, val in enumerate(series[window_size:]):\n","        mean = np.mean(series[idx:idx+window_size-1])\n","        preprocessed_series.append(mean+2*(val-mean))\n","    return np.concatenate((series[:window_size], preprocessed_series))\n","    \n","    \n","\n","random_indices = np.random.randint(0, 1999, (10,))\n","for x in random_indices:\n","    sample = np.asarray(df.loc[[x]].transpose()).squeeze()\n","    preprocessed_data = preprocess(sample)\n","    print(\"Raw vs. Preprocessed data sample {}\".format(x))\n","    plt.figure()\n","    plt.subplot(1,2,1)\n","    sns.lineplot(x=range(len(sample)), y=sample)\n","    plt.ylim(-2,2.3)\n","    plt.subplot(1,2,2)\n","    sns.lineplot(x=range(len(sample)), y=preprocessed_data)\n","    plt.ylim(-2,2.3)\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yqHDbmsiXzPE","colab_type":"text"},"cell_type":"markdown","source":["- As we can see by looking at some samples we can see that we can achieve the desired effect. The next step is to apply our model to the preprocessed data."]},{"metadata":{"scrolled":false,"id":"e6ckq59LXzPF","colab_type":"code","colab":{}},"cell_type":"code","source":["random_indices = np.random.randint(0, 1999, (30,))\n","for x in random_indices:\n","    sample = np.asarray(df.loc[[x]].transpose()).squeeze()\n","    preprocessed_data = preprocess(sample)\n","    bic_1, labels_1 = fit(sample, n_components=1)\n","    bic_2, labels_2 = fit(sample, n_components=2)\n","    if bic_1 < bic_2:\n","        pass\n","    else:\n","        print('================================================================================')\n","        print(\"Processing row {}\".format(x))\n","        print(\"Preprocessed + optimized Labels\")\n","        plot_classified(preprocessed_data, optimize_labels(labels_2))\n","        print(\"Not preprocessed + optimized labels\")\n","        plot_classified(sample, optimize_labels(labels_2))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qqPEw2aBXzPJ","colab_type":"text"},"cell_type":"markdown","source":["- As we can see in the results we can achieve a better separation by preprocessing the data. \n","- Even if the distributions overlap, we can distinguish two different gaussians since the difference in variance is amplified.\n","- As we have seen during the experiments the behvaviour which we have interpreted as \"normal\" have a mean close to 1. At least when looking at samples we can see the the anomalous behaviour has a mean that is further from 1 than the normal behaviour.\n","- We can use this assumption to preprocess the data even more, we can amplify the difference from 1 to achieve a clearer separation of means:"]},{"metadata":{"scrolled":false,"id":"_RD9r4m7XzPK","colab_type":"code","colab":{}},"cell_type":"code","source":["def preprocess_stage_2(series):\n","    assert(len(series) == 1200)\n","    series = preprocess(series)\n","    window_size = 10\n","    preprocessed_series = []\n","    for idx, val in enumerate(series[window_size:]):\n","        mean = np.mean(series[idx:idx+window_size-1])\n","        preprocessed_series.append(val+2*(mean-1))\n","    return np.concatenate((series[:window_size], preprocessed_series))\n","\n","random_indices = np.random.randint(0, 1999, (10,))\n","for x in random_indices:\n","    sample = np.asarray(df.loc[[x]].transpose()).squeeze()\n","    preprocessed_data = preprocess_stage_2(sample)\n","    print(\"Raw vs. Preprocessed data sample {}\".format(x))\n","    plt.figure()\n","    plt.subplot(1,2,1)\n","    sns.lineplot(x=range(len(sample)), y=sample)\n","    plt.ylim(-2,2.3)\n","    plt.subplot(1,2,2)\n","    sns.lineplot(x=range(len(sample)), y=preprocessed_data)\n","    plt.ylim(-2,2.3)\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nXGs3HAJXzPN","colab_type":"text"},"cell_type":"markdown","source":["- We can see that we can achieve the desired result. Now lets look at our predictions using the second stage preprocessing:"]},{"metadata":{"scrolled":false,"id":"HP6CcOXnXzPP","colab_type":"code","colab":{}},"cell_type":"code","source":["random_indices = np.random.randint(0, 1999, (30,))\n","for x in random_indices:\n","    sample = np.asarray(df.loc[[x]].transpose()).squeeze()\n","    preprocessed_data = preprocess_stage_2(sample)\n","    bic_1, labels_1 = fit(sample, n_components=1)\n","    bic_2, labels_2 = fit(sample, n_components=2)\n","    if bic_1 < bic_2:\n","        pass\n","    else:\n","        print('================================================================================')\n","        print(\"Processing row {}\".format(x))\n","        print(\"Preprocessed Stage 2 + optimized Labels\")\n","        plot_classified(preprocessed_data, optimize_labels(labels_2))\n","        print(\"Not preprocessed + optimized labels\")\n","        plot_classified(sample, optimize_labels(labels_2))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V1vyo6M4XzPZ","colab_type":"text"},"cell_type":"markdown","source":["- As we can see we have very few errors left. I think it would take a lot of effort to produce better results. At least one can build a prototype with this.\n","- Therefore we can look at the results in the other datasets:"]},{"metadata":{"id":"RQcj9twsXzPb","colab_type":"text"},"cell_type":"markdown","source":["## Train_Dataset_2.npy"]},{"metadata":{"scrolled":false,"id":"ZxSi_77mXzPc","colab_type":"code","colab":{}},"cell_type":"code","source":["df = pd.DataFrame(np.load('data/Train_Dataset_2.npy'))\n","\n","random_indices = np.random.randint(0, 1999, (30,))\n","for x in random_indices:\n","    sample = np.asarray(df.loc[[x]].transpose()).squeeze()\n","    preprocessed_data = preprocess_stage_2(sample)\n","    bic_1, labels_1 = fit(sample, n_components=1)\n","    bic_2, labels_2 = fit(sample, n_components=2)\n","    if bic_1 < bic_2:\n","        pass\n","    else:\n","        print('================================================================================')\n","        print(\"Processing row {}\".format(x))\n","        print(\"Preprocessed Stage 2 + optimized Labels\")\n","        plot_classified(preprocessed_data, optimize_labels(labels_2))\n","        print(\"Not preprocessed + optimized labels\")\n","        plot_classified(sample, labels_2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7q4vGSZRXzPg","colab_type":"text"},"cell_type":"markdown","source":["## Train_Dataset_3.npy"]},{"metadata":{"scrolled":false,"id":"K6tFIVXUXzPg","colab_type":"code","colab":{}},"cell_type":"code","source":["df = pd.DataFrame(np.load('data/Train_Dataset_2.npy'))\n","\n","random_indices = np.random.randint(0, 1999, (30,))\n","for x in random_indices:\n","    sample = np.asarray(df.loc[[x]].transpose()).squeeze()\n","    preprocessed_data = preprocess_stage_2(sample)\n","    bic_1, labels_1 = fit(sample, n_components=1)\n","    bic_2, labels_2 = fit(sample, n_components=2)\n","    if bic_1 < bic_2:\n","        pass\n","    else:\n","        print('================================================================================')\n","        print(\"Processing row {}\".format(x))\n","        print(\"Preprocessed Stage 2 + optimized Labels\")\n","        plot_classified(preprocessed_data, optimize_labels(labels_2))\n","        print(\"Not preprocessed + optimized labels\")\n","        plot_classified(sample, labels_2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1tVEjGV2XzPk","colab_type":"text"},"cell_type":"markdown","source":["- When we look at the results, we can see that we still make error as the difficulty of detecting the anomaly progresses. However I think using these predictions and clever heuristics we can find out by talking to the client or looking at the real operation we could achieve a sound solution, at least a solution that can be tested in the field and optimized after getting some more about the system performance.\n","- The model still fails when whe have a visually interpretable anomaly but the mean and variance of the distribution is really close. That means the GMM detects 2 modes, but they are so interleaved we can not clearly separate them. Here we would need to know if these modes that are so similar are actually anomalies or might be perturbations of the system or measuring device. Therefore it does not make sense to invest a lot of energy before testing this.\n","- Now the final heuristic we apply is based on the last optimization. When getting two different modes from the model we choose the values that have a mean further from 1 to be the anomaly.\n","- Of course we can still do a lot of optimization of hyperparameters etc.\n","\n","Now we can make the predictions for each dataset:"]},{"metadata":{"id":"U-b17QycXzPk","colab_type":"code","colab":{}},"cell_type":"code","source":["def fit_and_choose(sample):\n","    bic_1, labels_1 = fit(sample, n_components=1)\n","    bic_2, labels_2 = fit(sample, n_components=2)\n","    if bic_1 < bic_2:\n","        return labels_1\n","    return labels_2\n","\n","def translate_labels(sample_labels):\n","    mean_0 = np.mean(list(map(lambda x: x[0], filter(lambda x: x[1] == 0, zip(*sample_labels)))))\n","    mean_1 = np.mean(list(map(lambda x: x[0], filter(lambda x: x[1] == 1, zip(*sample_labels)))))\n","    if abs(mean_0-1) > abs(mean_1-1):\n","        result = np.invert(sample_labels[1].astype(np.bool))\n","        return result\n","    return sample_labels[1]\n","    \n","def make_predictions(filename):\n","    X = np.load('data/' + filename)\n","    preprocessed_X = np.apply_along_axis(preprocess_stage_2, 1, X)\n","    predicted_X = np.apply_along_axis(fit_and_choose, 1, preprocessed_X)\n","    optimized_X = np.apply_along_axis(optimize_labels, 1, predicted_X)\n","    preprocessed_optimized_X = zip(preprocessed_X, optimized_X)\n","    translated_X = list(map(translate_labels, preprocessed_optimized_X))\n","    np.save('predictions/' + filename, translated_X)\n","    \n","\n","make_predictions('Eval_Dataset_1.npy')\n","make_predictions('Eval_Dataset_2.npy')\n","make_predictions('Eval_Dataset_3.npy')"],"execution_count":0,"outputs":[]}]}